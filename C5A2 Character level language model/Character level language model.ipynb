{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo4gVJ_D01SH"
   },
   "source": [
    "# Character level language model - Dinosaurus Island\n",
    "\n",
    "Create new dinosaur names, by building a character-level language model to generate new names. The algorithm will learn the different name patterns, and randomly generate new names. \n",
    "\n",
    "* Store text data for processing using an RNN \n",
    "* Build a character-level text generation model using an RNN\n",
    "* Sample novel sequences in an RNN\n",
    "* Explain the vanishing/exploding gradient problem in RNNs\n",
    "* Apply gradient clipping as a solution for exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0Nj4psY01SJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_elAxqq01SN"
   },
   "source": [
    "#### Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB2XWVg_01SO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh3QcYpr01SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YltsxeZ01SU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: '\\n',\n",
      "    1: 'a',\n",
      "    2: 'b',\n",
      "    3: 'c',\n",
      "    4: 'd',\n",
      "    5: 'e',\n",
      "    6: 'f',\n",
      "    7: 'g',\n",
      "    8: 'h',\n",
      "    9: 'i',\n",
      "    10: 'j',\n",
      "    11: 'k',\n",
      "    12: 'l',\n",
      "    13: 'm',\n",
      "    14: 'n',\n",
      "    15: 'o',\n",
      "    16: 'p',\n",
      "    17: 'q',\n",
      "    18: 'r',\n",
      "    19: 's',\n",
      "    20: 't',\n",
      "    21: 'u',\n",
      "    22: 'v',\n",
      "    23: 'w',\n",
      "    24: 'x',\n",
      "    25: 'y',\n",
      "    26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yYvYeI501SX"
   },
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "\n",
    "    # Clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]\n",
    "    for gradient in gradients:\n",
    "        np.clip(gradients[gradient], -maxValue, maxValue, out = gradients[gradient])\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIkYdtBx01Su"
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Step 1: Create the a zero vector x that can be used as the one-hot vector \n",
    "    # Representing the first character (initializing the sequence generation)\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # Initialize a_prev as zeros\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices. This is the list which will contain the list of indices of the characters to generate\n",
    "    indices = []\n",
    "    \n",
    "    # idx is the index of the one-hot vector x that is set to 1\n",
    "    # All other positions in x are zero.\n",
    "    # Initialize idx to -1\n",
    "    idx = -1\n",
    "    \n",
    "    # Loop over time-steps t. At each time-step:\n",
    "    # Sample a character from a probability distribution \n",
    "    # And append its index (`idx`) to the list \"indices\". \n",
    "    # Stop if you reach 50 characters (which should be very unlikely with a well-trained model).\n",
    "    # Setting the maximum number of characters helps with debugging and prevents infinite loops. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):     \n",
    "        # Forward propagate x\n",
    "        a = np.tanh(Waa @ a_prev + Wax @ x + b)\n",
    "        z = Wya @ a + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # For grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Overwrite the input x with one that corresponds to the sampled index `idx`.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BbEdIgY01S3"
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    # Forward propagate through time\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l214uNun01S_"
   },
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    examples = [x.strip() for x in data_x]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # for grading purposes\n",
    "    last_dino_name = \"abc\"\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):     \n",
    "        # Set the index `idx`\n",
    "        idx = (len(examples) + j) %  len(examples)\n",
    "        \n",
    "        # Set the input X\n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [c for c in single_example]\n",
    "        single_example_ix = [char_to_ix[i] for i in single_example_chars]\n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        # Set the labels Y\n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:] + [ix_newline]\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "      \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        # to keep the loss smooth.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                # To get the same result (for grading purposes), increment the seed by one\n",
    "                seed += 1  \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = turiasaurus\n",
      "single_example_chars ['t', 'u', 'r', 'i', 'a', 's', 'a', 'u', 'r', 'u', 's']\n",
      "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
      " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382339\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.248243\n",
      "\n",
      "Meustolong\n",
      "Indabesbacaptos\n",
      "Iusspancimathestarliabpantevha\n",
      "Macacosaurus\n",
      "Yusocops\n",
      "Cabasodeditisaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.879534\n",
      "\n",
      "Physsphonosaurus\n",
      "Midaaasthachus\n",
      "Myssrisaurus\n",
      "Phaajstegasthus\n",
      "Yuspenosaurus\n",
      "Eialosaurus\n",
      "Tromicosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.893040\n",
      "\n",
      "Onyushangosaurus\n",
      "Lohaaerrhachykuraptor\n",
      "Lyusocops\n",
      "Olbagrona\n",
      "Yusocops\n",
      "Eiaerosaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.778439\n",
      "\n",
      "Piuspteres\n",
      "Midealosaurus\n",
      "Mustoimankeurus\n",
      "Pedadosaurus\n",
      "Yusocia\n",
      "Eiagosaurus\n",
      "Trodonimateritaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u55TY-W01TG"
   },
   "source": [
    "# Writing like Shakespeare\n",
    "\n",
    "Using LSTM cells, we can learn longer-term dependencies that span many characters in the text--e.g., where a character appearing somewhere a sequence can influence what should be a different character, much later in the sequence. These long-term dependencies were less important with dinosaur names, since the names were quite short. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgPMYCZl01TH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRaDcXrU01TJ"
   },
   "source": [
    "The model has already been trained for ~1000 epochs on a collection of Shakespearean poems called \"<i>[The Sonnets](shakespeare.txt)</i>.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4DBjTlz01TK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 102s 401ms/step - loss: 2.5403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f7b638ab88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDoriy4Z01TM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: Forsooth this maketh no sense\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "Forsooth this maketh no sense,\n",
      "when in heart besinds cife anfored witine.\n",
      "how this hase tonbe tume in of denerter,\n",
      "ald foat me sto shast thy som doth haw heop me, my i erling.\n",
      "thou ald the ryupy crearaoses roklied doth,\n",
      "and not askich porter that so the ansien,\n",
      "shu on cent bed hils ang mening brigat,\n",
      "and worth my very oe thise in theid,\n",
      "bitast meviroing senfeed me i achaons,\n",
      "but sighes foor, his neved thing grothy,\n",
      "nor ary th"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dinosaurus_Island_Character_level_language_model_final_v3b+Proposed.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "DLSC5W1-A2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
