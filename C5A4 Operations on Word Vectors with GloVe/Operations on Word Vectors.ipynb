{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTNn4zzAJWhA"
   },
   "source": [
    "# Operations on Word Vectors\n",
    "\n",
    "* Explain how word embeddings capture relationships between words\n",
    "* Load pre-trained word vectors\n",
    "* Measure similarity between word vectors using cosine similarity\n",
    "* Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cWEywjgpJWhM",
    "outputId": "6b0b856a-d05d-4a6d-ba94-01d247254f20"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from w2v_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-CfRQ86JWhN"
   },
   "source": [
    "#### Load the Word Vectors\n",
    "using 50-dimensional GloVe vectors to represent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xCsAgH3dJWhO"
   },
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZBg2QwSJWhO"
   },
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "To measure the similarity between two words, we need a way to measure the degree of similarity between two embedding vectors for the two words.\n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mNjuTQ5JJWhP"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    # Special case. Consider the case u = [0, 0], v=[0, 0]\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "\n",
    "    # Compute the dot product between u and v\n",
    "    dot = np.dot(u, v)\n",
    "    # Compute the L2 norm of u\n",
    "    norm_u = np.sqrt(np.sum(np.square(u)))\n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(np.square(v)))\n",
    "    \n",
    "    # Avoid division by 0\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63Pjp_QSJWhQ"
   },
   "source": [
    "#### Word Analogy Task\n",
    "trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n",
    "    $e_b - e_a \\approx e_d - e_c$   (using cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kGBV3yoQJWhS"
   },
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    # convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b and e_c\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "    \n",
    "    # loop over the whole word vector set\n",
    "    for w in words:   \n",
    "        # to avoid best_word being one the input words, skip the input word_c\n",
    "        # skip word_c from query\n",
    "        if w == word_c:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> smaller\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMRD25MuJWhW"
   },
   "source": [
    "### Debiasing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_qpU-C3KJWhW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
      " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
      "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
      "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
      "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
      " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
      " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
      "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
      " -0.04371     0.01258   ]\n"
     ]
    }
   ],
   "source": [
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TgqV6pDxJWhX",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "john -0.23163356145973724\n",
      "marie 0.315597935396073\n",
      "sophie 0.3186878985941878\n",
      "ronaldo -0.31244796850329437\n",
      "priya 0.17632041839009402\n",
      "rahul -0.16915471039231722\n",
      "danielle 0.24393299216283895\n",
      "reza -0.07930429672199553\n",
      "katy 0.2831068659572615\n",
      "yasmin 0.23313857767928753\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wgadfCaGJWhY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "lipstick 0.27691916256382665\n",
      "guns -0.1888485567898898\n",
      "science -0.06082906540929699\n",
      "arts 0.008189312385880344\n",
      "literature 0.0647250443345993\n",
      "warrior -0.20920164641125288\n",
      "doctor 0.11895289410935045\n",
      "tree -0.07089399175478092\n",
      "receptionist 0.3307794175059374\n",
      "technology -0.13193732447554293\n",
      "fashion 0.035638946257727\n",
      "teacher 0.1792092343182567\n",
      "engineer -0.08039280494524072\n",
      "pilot 0.0010764498991917074\n",
      "computer -0.10330358873850498\n",
      "singer 0.18500518136496297\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUzfNtq4JWhY"
   },
   "source": [
    "these results reflect certain unhealthy gender stereotypes.\n",
    "\n",
    "#### Neutralize Bias for Non-Gender Specific Words \n",
    "If you're using a 50-dimensional word embedding, the 50 dimensional space can be split into two parts: The bias-direction $g$, and the remaining 49 dimensions, which is called $g_{\\perp}$ here.\n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\\tag{2}$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}\\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "79Pk0QDhJWhZ"
   },
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map.\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula given above.\n",
    "    e_biascomponent = (np.dot(e, g) / np.sum(np.square(g))) * g\n",
    " \n",
    "    # Neutralize e by subtracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6PgkxwxXJWhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.3307794175059374\n",
      "cosine similarity between receptionist and g, after neutralizing:  -4.442232511624783e-17\n"
     ]
    }
   ],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciATgxwLJWha"
   },
   "source": [
    "#### Equalization Algorithm for Gender-Specific Words\n",
    "\n",
    "Equalization is applied to pairs of words that you might want to have differ only through the gender property. The key idea behind equalization is to make sure that a particular pair of words are equidistant from the 49-dimensional $g_\\perp$.\n",
    "\n",
    "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}\\tag{4}$$ \n",
    "\n",
    "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{5}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B} \\tag{6}$$\n",
    "\n",
    "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{7}$$ \n",
    "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{8}$$\n",
    "\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||_2} \\tag{9}$$\n",
    "\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||_2} \\tag{10}$$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} \\tag{11}$$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} \\tag{12}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aBhxJtGIJWha"
   },
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    # Select word vector representation of \"word\"\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Compute the mean of e_w1 and e_w2\n",
    "    mu = (e_w1 + e_w2) /2\n",
    "\n",
    "    # Compute the projections of mu over the bias axis and the orthogonal axis\n",
    "    mu_B = (np.dot(mu, bias_axis) / np.sum(np.square(bias_axis))) * bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Use equations (7) and (8) to compute e_w1B and e_w2B\n",
    "    e_w1B = (np.dot(e_w1, bias_axis) / np.sum(np.square(bias_axis))) * bias_axis\n",
    "    e_w2B = (np.dot(e_w2, bias_axis) / np.sum(np.square(bias_axis))) * bias_axis\n",
    "        \n",
    "    # Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above\n",
    "    corrected_e_w1B = np.sqrt(abs(1 - np.sum(np.square(mu_orth)))) * ((e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B))\n",
    "    corrected_e_w2B = np.sqrt(abs(1 - np.sum(np.square(mu_orth)))) * ((e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B))\n",
    "\n",
    "    # Debias by equalizing e1 and e2 to the sum of their corrected projections\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "\n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "P405J5ZSJWhb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.11711095765336832\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.35666618846270376\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.7004364289309388\n",
      "cosine_similarity(e2, gender) =  0.7004364289309389\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W2-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
